{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testing-ForPatent-Prior-Art-Finder_Gabe.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMW1uToifT1pVjsXwYB6gaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabeAspir/Patent-Prior-Art-Finder/blob/main/Testing_ForPatent_Prior_Art_Finder_Gabe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJWOZJAegI5"
      },
      "source": [
        "# Goal -- To Test!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JVZkmOJ6EHo"
      },
      "source": [
        "Idk yet how to import a python file on my local machine to colab yet-- so I copied and pasted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEPB3JUo5xL_"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "\n",
        "\n",
        "# Gabe\n",
        "def init(csv, publicationNumberColumnString, comparisonColumnString):\n",
        "    # Column Headers for dataframe:\n",
        "    # PublicationNumber #Abstract\n",
        "    # Dataframe will be created\n",
        "\n",
        "    dataframe = pd.read_csv(csv)\n",
        "    dataframe.rename(columns={publicationNumberColumnString: 'PublicationNumber'}, inplace=True)\n",
        "    dataframe.rename(columns={comparisonColumnString: 'Abstract'}, inplace=True)\n",
        "\n",
        "    _tokenize(dataframe)\n",
        "    corpus = _createCorpus(dataframe)\n",
        "    _bagOfWordize(dataframe, corpus)\n",
        "    _TFIDFize(dataframe, corpus)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "# Private methods for init to call\n",
        "# Gabe\n",
        "def _tokenize(dataframe):\n",
        "    dataframe['Tokens'] = dataframe['Abstract'].apply(_tokenizeText)\n",
        "\n",
        "\n",
        "# Will add column to dataframe called 'Tokens'\n",
        "# Gabe\n",
        "def _tokenizeText(string):\n",
        "\n",
        "\n",
        "    def filterOut(word):\n",
        "        remove_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
        "                       'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
        "                       'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his',\n",
        "                       'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
        "                       \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
        "                       'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
        "                       \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was',\n",
        "                       'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
        "                       'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
        "                       'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "                       'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
        "                       'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
        "                       'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
        "                       'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
        "                       'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
        "                       'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
        "                       'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
        "                       'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o',\n",
        "                       're', 've', 'y', 'ain', 'aren', \"aren't\",\n",
        "                       'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',\n",
        "                       \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\",\n",
        "                       'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
        "                       \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
        "                       'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\",\n",
        "                       'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "        if word in remove_list:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    lowercasedString = string.lower()\n",
        "    # To split based on white space and random characters\n",
        "    stringArray = re.split('\\W+', lowercasedString)\n",
        "    # Will substitute numbers for _NUM_\n",
        "    stringArray = [re.sub(r\"[0-9]+\", \"_NUM_\", s) for s in stringArray]\n",
        "    # Will filter out 1 letter words like \"I\" and \"a\"\n",
        "    stringArray = list(filter(lambda s: len(s) > 1, stringArray))\n",
        "    fullyFiltered = list(filter(filterOut, stringArray))\n",
        "    # Will return a List/Array\n",
        "    return fullyFiltered\n",
        "\n",
        "\n",
        "# Gabe\n",
        "def _createCorpus(dataframe):\n",
        "    corpus = []\n",
        "\n",
        "    for i in dataframe.index:\n",
        "        tokens = dataframe['Tokens'][i]\n",
        "\n",
        "        # Only adds the new words by converting the lists into sets (no doubles)\n",
        "        # Then finding the new words by subtracting one set (a) from another set (b)\n",
        "        # then adds back in the new words that were in (b) and not in (a), back into a\n",
        "        token_set = set(tokens)\n",
        "        corpus_set = set(corpus)\n",
        "        new_tokens = token_set - corpus_set\n",
        "        corpus = corpus + list(new_tokens)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "# Zach\n",
        "def _createNewCorpus(dataframe, newTokens):\n",
        "    corpus = []\n",
        "\n",
        "    for i in dataframe.index:\n",
        "        tokens = dataframe['Tokens'][i]\n",
        "\n",
        "        # Only adds the new words by converting the lists into sets (no doubles)\n",
        "        # Then finding the new words by subtracting one set (a) from another set (b)\n",
        "        # then adds back in the new words that were in (b) and not in (a), back into a\n",
        "        token_set = set(tokens)\n",
        "        corpus_set = set(corpus)\n",
        "        new_tokens = token_set - corpus_set\n",
        "        corpus = corpus + list(new_tokens)\n",
        "\n",
        "    # doing the same thing with the new tokens\n",
        "    corpus_set = set(corpus)\n",
        "    token_set = set(newTokens)  # set of the newTokens from the parameter\n",
        "    new_tokens = token_set - corpus_set\n",
        "\n",
        "    corpus = corpus + list(new_tokens)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "# Ephraim\n",
        "# Will add column called 'BagOfWords' to dataframe\n",
        "def _bagOfWordize(dataframe, corpus):\n",
        "    counts = []\n",
        "    for row in dataframe['Tokens']:\n",
        "        count = []  # Initialize count as an empty list\n",
        "        for word in corpus:\n",
        "            count.append(row.count(word))  # get the wordcount in each list of words, and record the count\n",
        "        counts.append(\n",
        "            count)  # Each list of wordCount vectors represents one document, and the counts variable is the list of all our docs' counts\n",
        "    dataframe['BagOfWords'] = counts\n",
        "\n",
        "\n",
        "# Zach\n",
        "def _TFIDFize(dataframe, corpus):\n",
        "    # adding column called 'TF-IDF'\n",
        "    dataframe.insert(len(dataframe.columns), 'TF-IDF', '')\n",
        "\n",
        "    # for each set of tokens, creates a vector of tf-idf values and adds it to the new column\n",
        "    for i in dataframe.index:\n",
        "        tokens = dataframe['Tokens'][i]\n",
        "        vector = _vectorize_tf_idf(dataframe, tokens, corpus)\n",
        "        dataframe['TF-IDF'][i] = vector\n",
        "\n",
        "\n",
        "def _vectorize_tf_idf(data, tokens, corpus):\n",
        "    v = []\n",
        "    for word in corpus:\n",
        "        # tf: number of times word appears in tokens for this abstract over the amounf of (tokenized) words in the patent\n",
        "        tf = tokens.count(word) / len(tokens)\n",
        "        number_of_patents_with_word = _appearences(data, word)\n",
        "\n",
        "        # idf: the log of the amount of documents divided by the number of patents with the word\n",
        "        idf = math.log(float(len(data)) / number_of_patents_with_word)\n",
        "        v.append(tf * idf)\n",
        "    return v\n",
        "\n",
        "\n",
        "def _appearences(data, word):\n",
        "    # gets the number of times a word appears in the tokens of all the data\n",
        "    number = 0\n",
        "    for tokens in data['Tokens']:\n",
        "        if word in tokens:\n",
        "            number += 1\n",
        "\n",
        "    return number\n",
        "\n",
        "\n",
        "# For the User\n",
        "# Must Initialize first\n",
        "\n",
        "# Zach\n",
        "def jaccardTable(dataframe):\n",
        "    table = pd.DataFrame(dataframe['PublicationNumber'])  # creating a new table\n",
        "    for bow, n in zip(dataframe['BagOfWords'], dataframe['PublicationNumber']):  # iterating through both at same time\n",
        "        number = n  # getting the publication number so can use it as header later on\n",
        "        comps = []  # series that represents this bag of word's cosine comp with all bow's\n",
        "        for b in dataframe['BagOfWords']:  # getting the other bag of words\n",
        "            comps.append(jaccardSimilarity(bow, b))  # applying jaccard similarity to the 2 BOWs\n",
        "        table[n] = comps  # adding this new column, n is the publication number from above\n",
        "\n",
        "    return table\n",
        "\n",
        "\n",
        "# accepts vector bag of words\n",
        "def jaccardSimilarity(patent1, patent2):\n",
        "    count = 0\n",
        "\n",
        "    # counting the number of total words combined between both of them\n",
        "    for x in range(len(patent1)):\n",
        "        if patent1[x] != 0 or patent2[x] != 0:  # not equaling 0 means that it occurs at least once\n",
        "            count += 1\n",
        "    numerator = 0\n",
        "\n",
        "    # Counting the number of words in both\n",
        "    for x in range(len(patent1)):\n",
        "        if patent1[x] != 0 and patent2[x] != 0:\n",
        "            numerator += 1\n",
        "\n",
        "    return (numerator / count)\n",
        "\n",
        "\n",
        "# Zach\n",
        "def cosineSimilarity(patent1, patent2):\n",
        "    v1 = np.array(patent1).reshape(1, -1)\n",
        "    v2 = np.array(patent2).reshape(1, -1)\n",
        "    return cosine_similarity(v1, v2)[0][0]\n",
        "\n",
        "\n",
        "def cosineTable(dataframe):\n",
        "    newTable = pd.DataFrame(cosine_similarity(dataframe['BagOfWords']))\n",
        "    newTable.columns = dataframe['Publication_Number']\n",
        "    newTable.index = dataframe['Publication_Number']\n",
        "\n",
        "    return newTable\n",
        "\n",
        "\n",
        "# Zach\n",
        "# Comparing new patent based on TF-IDF/Cosine Similarity\n",
        "# dataframe must have TF-IDF column\n",
        "def compareNewPatent(newComparisonText, dataframe):\n",
        "    text = _tokenizeText(newComparisonText)\n",
        "    new_tokens = _tokenizeText(newComparisonText)\n",
        "    new_corpus = _createCorpus(dataframe, new_tokens)  # has to create new corpus\n",
        "    new_vector = _vectorize_tf_idf(dataframe, new_tokens,\n",
        "                                   new_corpus)  # gets a vector with the tf-idf values of new text\n",
        "\n",
        "    tuples = []\n",
        "\n",
        "    for pn, vec in zip(dataframe['PublicationNumber'],\n",
        "                       dataframe['TF-IDF']):  # iterates through both of these columns at same time\n",
        "        similarity = cosineSimilarity(new_vector, vec)  # compares new TF-IDF vector to the ones in dataframe\n",
        "        tuples.append([pn, similarity])  # adds to the tuples, contains the patent number and similarity\n",
        "    tuples = sorted(tuples, key=lambda similarity: similarity[1],\n",
        "                    reverse=True)  # sort the tuples based off of similarity\n",
        "    df = pd.DataFrame(tuples,\n",
        "                      columns=['Publication Number', 'Similarity'])  # turns the sorted tuple into a pandas dataframe\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpQaswmZ51o5"
      },
      "source": [
        "url = 'https://drive.google.com/file/d/18DdQd4ZPbcvOeZ6x2KRJHmvpGarnw9Qx/view?usp=sharing'\n",
        "file_id = url.split('/')[-2]\n",
        "csv1='https://drive.google.com/uc?id=' + file_id"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "wCWfZacL6MxL",
        "outputId": "61dd0b38-5e02-4f44-fd89-0a11328d5d08"
      },
      "source": [
        "myDataFrame = init(csv1, 'Publication_Number', 'Abstract')\n",
        "myDataFrame"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PublicationNumber</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Description</th>\n",
              "      <th>Claim</th>\n",
              "      <th>Tokens</th>\n",
              "      <th>BagOfWords</th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US-7365259-B2</td>\n",
              "      <td>A keyboard apparatus is including a plurality ...</td>\n",
              "      <td>BACKGROUND OF THE INVENTION \\n   1. Field of t...</td>\n",
              "      <td>1. A keyboard apparatus comprising:\\n a suppor...</td>\n",
              "      <td>[keyboard, apparatus, including, plurality, ke...</td>\n",
              "      <td>[2, 1, 1, 1, 2, 1, 1, 2, 4, 1, 1, 2, 2, 2, 1, ...</td>\n",
              "      <td>[0.04598394035526001, 0.017199611490370515, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US-7556524-B2</td>\n",
              "      <td>An easy-pull type swivel plug includes a body,...</td>\n",
              "      <td>BACKGROUND OF THE INVENTION \\n   1. Field of t...</td>\n",
              "      <td>1. An easy-pull type swivel plug, comprising:\\...</td>\n",
              "      <td>[easy, pull, type, swivel, plug, includes, bod...</td>\n",
              "      <td>[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, ...</td>\n",
              "      <td>[0.0, 0.038221358867490035, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US-7338315-B2</td>\n",
              "      <td>The invention relates to a closure device comp...</td>\n",
              "      <td>FIELD OF THE INVENTION \\n   The invention rela...</td>\n",
              "      <td>1. A closure device comprising:\\n a wall havin...</td>\n",
              "      <td>[invention, relates, closure, device, comprisi...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, ...</td>\n",
              "      <td>[0.0, 0.0, 0.025958676007001618, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US-6843642-B2</td>\n",
              "      <td>An air compressor with shock-absorption rubber...</td>\n",
              "      <td>BACKGROUND OF THE INVENTION \\n   (a) Field of ...</td>\n",
              "      <td>1. An air compressor with shock-absorption rub...</td>\n",
              "      <td>[air, compressor, shock, absorption, rubber, s...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US-9433212-B2</td>\n",
              "      <td>Provided is a novel plant growth regulator. Th...</td>\n",
              "      <td>TECHNICAL FIELD \\n     The present invention r...</td>\n",
              "      <td>The invention claimed is: \\n     \\n       1. A...</td>\n",
              "      <td>[provided, novel, plant, growth, regulator, di...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>US-5536436-A</td>\n",
              "      <td>A liquid laundry detergent composition contain...</td>\n",
              "      <td>FIELD OF THE INVENTION \\n     The present inve...</td>\n",
              "      <td>What is claimed is: \\n     \\n       1. A heavy...</td>\n",
              "      <td>[liquid, laundry, detergent, composition, cont...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>US-2015111807-A1</td>\n",
              "      <td>A liquid laundry detergent composition compris...</td>\n",
              "      <td>FIELD OF THE INVENTION \\n     The present inve...</td>\n",
              "      <td>What is claimed is: \\n     \\n         1 . A li...</td>\n",
              "      <td>[liquid, laundry, detergent, composition, comp...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
              "      <td>[0.0, 0.03344368900905378, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>US-7605322-B2</td>\n",
              "      <td>As a player inputs a performance of a music pi...</td>\n",
              "      <td>TECHNICAL FIELD \\n   The present invention rel...</td>\n",
              "      <td>1. An apparatus for automatically starting an ...</td>\n",
              "      <td>[player, inputs, performance, music, piece, pl...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.021459172165788003, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>US-7205268-B2</td>\n",
              "      <td>A low-foaming aqueous liquid laundry detergent...</td>\n",
              "      <td>FIELD OF THE INVENTION \\n     The present inve...</td>\n",
              "      <td>1. A low-foaming aqueous liquid laundry deterg...</td>\n",
              "      <td>[low, foaming, aqueous, liquid, laundry, deter...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>US-6910186-B2</td>\n",
              "      <td>A new class of avatars (â€œorganizational avatar...</td>\n",
              "      <td>CROSS-REFERENCE TO APPENDICES ATTACHED HERETO ...</td>\n",
              "      <td>1. A method of communicating between users, th...</td>\n",
              "      <td>[new, class, avatars, organizational, avatars,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  PublicationNumber  ...                                             TF-IDF\n",
              "0     US-7365259-B2  ...  [0.04598394035526001, 0.017199611490370515, 0....\n",
              "1     US-7556524-B2  ...  [0.0, 0.038221358867490035, 0.0, 0.0, 0.0, 0.0...\n",
              "2     US-7338315-B2  ...  [0.0, 0.0, 0.025958676007001618, 0.0, 0.0, 0.0...\n",
              "3     US-6843642-B2  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "4     US-9433212-B2  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "5      US-5536436-A  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "6  US-2015111807-A1  ...  [0.0, 0.03344368900905378, 0.0, 0.0, 0.0, 0.0,...\n",
              "7     US-7605322-B2  ...  [0.021459172165788003, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
              "8     US-7205268-B2  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "9     US-6910186-B2  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJH984g56mk9"
      },
      "source": [
        "initialization complete!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "t4jemt1H6o8N",
        "outputId": "e28d8ca2-96a4-4925-abb3-1e060680b648"
      },
      "source": [
        "jaccardTable(myDataFrame)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PublicationNumber</th>\n",
              "      <th>US-7365259-B2</th>\n",
              "      <th>US-7556524-B2</th>\n",
              "      <th>US-7338315-B2</th>\n",
              "      <th>US-6843642-B2</th>\n",
              "      <th>US-9433212-B2</th>\n",
              "      <th>US-5536436-A</th>\n",
              "      <th>US-2015111807-A1</th>\n",
              "      <th>US-7605322-B2</th>\n",
              "      <th>US-7205268-B2</th>\n",
              "      <th>US-6910186-B2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US-7365259-B2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.037975</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US-7556524-B2</td>\n",
              "      <td>0.037975</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.064935</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US-7338315-B2</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>0.064935</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.049180</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014493</td>\n",
              "      <td>0.016949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US-6843642-B2</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.049180</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US-9433212-B2</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>US-5536436-A</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.120690</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.026549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>US-2015111807-A1</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.120690</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>US-7605322-B2</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>US-7205268-B2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014493</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.028571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>US-6910186-B2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026549</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028571</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  PublicationNumber  US-7365259-B2  ...  US-7205268-B2  US-6910186-B2\n",
              "0     US-7365259-B2       1.000000  ...       0.000000       0.000000\n",
              "1     US-7556524-B2       0.037975  ...       0.000000       0.000000\n",
              "2     US-7338315-B2       0.078947  ...       0.014493       0.016949\n",
              "3     US-6843642-B2       0.032258  ...       0.019608       0.000000\n",
              "4     US-9433212-B2       0.020000  ...       0.000000       0.000000\n",
              "5      US-5536436-A       0.000000  ...       0.137931       0.026549\n",
              "6  US-2015111807-A1       0.029851  ...       0.140000       0.000000\n",
              "7     US-7605322-B2       0.040000  ...       0.000000       0.000000\n",
              "8     US-7205268-B2       0.000000  ...       1.000000       0.028571\n",
              "9     US-6910186-B2       0.000000  ...       0.028571       1.000000\n",
              "\n",
              "[10 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "tc7u3qNt-5kh",
        "outputId": "9efd7a21-9c3d-4763-b695-17fef85a313f"
      },
      "source": [
        "cosineTable(myDataFrame)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-49f6abf8efcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosineTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-ad36df8e09d7>\u001b[0m in \u001b[0;36mcosineTable\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosineTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mnewTable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BagOfWords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0mnewTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Publication_Number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mnewTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Publication_Number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    135\u001b[0m         X = Y = check_array(X, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m    136\u001b[0m                             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             estimator=estimator)\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         X = check_array(X, accept_sparse=accept_sparse, dtype=dtype,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    }
  ]
}